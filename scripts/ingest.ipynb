{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24a1817e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import pprint\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers import util\n",
    "from transformers import AutoTokenizer\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "from src.utils import build_metadata_blurb, search_articles\n",
    "from src.mem import check_memory\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "# torch.set_float32_matmul_precision('high') # bloat32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81c0ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "with open('../data/articles.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# instantiate model + tokenizer\n",
    "model = SentenceTransformer(\"joe32140/ModernBERT-base-msmarco\") # longer context window, can avoid chunking since max article len is 6600 toks\n",
    "tok = AutoTokenizer.from_pretrained(\"joe32140/ModernBERT-base-msmarco\")\n",
    "\n",
    "# config \n",
    "max_length = model.max_seq_length # 8192 toks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb72ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 38\n",
      "max: 6676\n",
      "mean: 1343.1263636363637\n",
      "median: 1165.5\n"
     ]
    }
   ],
   "source": [
    "# # check if truncation will be a problem (we're good!)\n",
    "# import statistics as s \n",
    "# tok_len = [len(tok.encode(rec['bodyText'])) for rec in data]\n",
    "# print(f\"min: {min(tok_len)}\\nmax: {max(tok_len)}\\nmean: {s.mean(tok_len)}\\nmedian: {s.median(tok_len)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3371bed9",
   "metadata": {},
   "source": [
    "# Data processing \n",
    "Here, I pre-process input data, creating embeddings for key article metadata + the body text. I'll average the two tensors, slightly upweighting key, \"high-signal\" information (e.g. hed, tags, summary) to project each article into representation space. \n",
    "\n",
    "I keep these separate initially to **better preserve structure/signal from metadata.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15f2b2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# meta-data\n",
    "meta = build_metadata_blurb(data)\n",
    "meta_embeddings = model.encode(meta, convert_to_tensor = True)\n",
    "\n",
    "# core text \n",
    "body_text = [rec['bodyText'] for rec in data]\n",
    "body_text_embeddings = model.encode(body_text, convert_to_tensor = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76dc240d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg. embeddings to create a ~unified representation~ :) \n",
    "# this is lossier than concat, but faster and more compact dimension-wise – also good for control over \n",
    "# info salience (via alpha) + semantic coherence \n",
    "alpha = 0.7 # there's more principled ways to choose this lol\n",
    "combined_embeddings = alpha * meta_embeddings + (1- alpha) * body_text_embeddings # dim: 1100 x 768\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "257ec792",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pd.DataFrame(data)\n",
    "\n",
    "# save to persistent volume – fine for now since it's a small dataset, but in production/when complexity tradeoff \n",
    "# makes sense (10,000+ docs), you could do a faiss-based approach: https://github.com/facebookresearch/faiss\n",
    "torch.save(combined_embeddings, '../data/article_embeddings.pt')\n",
    "meta.to_parquet('../data/article_metadata.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a2d8cd",
   "metadata": {},
   "source": [
    "# Initial test\n",
    "\n",
    "Basic test of search to check embedding quality + such. Have filters to handle some basic failure modes (e.g. few relevant results.)\n",
    "\n",
    "To improve this, you could normalize scores globally to improve differentiation across documents. In production, some ways to handle this include: \n",
    "<ul>1. <b>multi-stage retrieval:</b> fast retrieval then re-ranking (cross-encoders, folding in info abt popularity/user prefs)</ul>\n",
    "<ul>2. <b>hybrid approach:</b> weighted combo of keyword + semantic search scores</ul>\n",
    "\n",
    "Can also implement something like [query expansion](https://research.google/pubs/learning-for-efficient-supervised-query-expansion-via-two-stage-feature-selection/), by adding in related tokens/phrases to cast a wider net (I'd have done this w/ qwen or something, but considering df size thought it'd be overkill and/or tough to dial in meaningfully.\n",
    "\n",
    "Less important, but maybe add a check for ood user behavior (e.g. \"what's 2+2\") – that's dependent on final intended use-case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5be645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src\n",
    "importlib.reload(src.utils)\n",
    "from src.utils import search_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98eeb118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance_score</th>\n",
       "      <th>headline</th>\n",
       "      <th>summary</th>\n",
       "      <th>timesTags</th>\n",
       "      <th>firstPublished</th>\n",
       "      <th>url</th>\n",
       "      <th>tone</th>\n",
       "      <th>typeOfMaterials</th>\n",
       "      <th>bylines</th>\n",
       "      <th>bodyText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1060</th>\n",
       "      <td>0.439156</td>\n",
       "      <td>Environmental Changes Are Fueling Human, Anima...</td>\n",
       "      <td>Biodiversity loss, global warming, pollution a...</td>\n",
       "      <td>[your-feed-science, your-feed-health, your-fee...</td>\n",
       "      <td>2024-05-08T15:00:24.000Z</td>\n",
       "      <td>https://www.nytimes.com/2024/05/08/health/envi...</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>[News]</td>\n",
       "      <td>By Emily Anthes</td>\n",
       "      <td>Several large-scale, human-driven changes to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1045</th>\n",
       "      <td>0.374678</td>\n",
       "      <td>Alarmed by Climate Change, Astronomers Train T...</td>\n",
       "      <td>A growing number of researchers in the field a...</td>\n",
       "      <td>[Global Warming, Space and Astronomy, Earth, A...</td>\n",
       "      <td>2024-05-14T07:00:24.000Z</td>\n",
       "      <td>https://www.nytimes.com/2024/05/14/science/ast...</td>\n",
       "      <td>NEWS</td>\n",
       "      <td>[News]</td>\n",
       "      <td>By Katrina Miller and Delger Erdenesanaa</td>\n",
       "      <td>On the morning of Jan. 18, 2003, Penny Sackett...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      relevance_score                                           headline  \\\n",
       "1060         0.439156  Environmental Changes Are Fueling Human, Anima...   \n",
       "1045         0.374678  Alarmed by Climate Change, Astronomers Train T...   \n",
       "\n",
       "                                                summary  \\\n",
       "1060  Biodiversity loss, global warming, pollution a...   \n",
       "1045  A growing number of researchers in the field a...   \n",
       "\n",
       "                                              timesTags  \\\n",
       "1060  [your-feed-science, your-feed-health, your-fee...   \n",
       "1045  [Global Warming, Space and Astronomy, Earth, A...   \n",
       "\n",
       "                firstPublished  \\\n",
       "1060  2024-05-08T15:00:24.000Z   \n",
       "1045  2024-05-14T07:00:24.000Z   \n",
       "\n",
       "                                                    url  tone typeOfMaterials  \\\n",
       "1060  https://www.nytimes.com/2024/05/08/health/envi...  NEWS          [News]   \n",
       "1045  https://www.nytimes.com/2024/05/14/science/ast...  NEWS          [News]   \n",
       "\n",
       "                                       bylines  \\\n",
       "1060                           By Emily Anthes   \n",
       "1045  By Katrina Miller and Delger Erdenesanaa   \n",
       "\n",
       "                                               bodyText  \n",
       "1060  Several large-scale, human-driven changes to t...  \n",
       "1045  On the morning of Jan. 18, 2003, Penny Sackett...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# climate results \n",
    "clim = search_articles(model, \n",
    "                          \"climate change\", \n",
    "                          combined_embeddings, \n",
    "                          meta, \n",
    "                          5,\n",
    "                          0.3,\n",
    "                          sort_key = 'relevance_score')\n",
    "\n",
    "clim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e61d2d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'no relevant results!'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# irrelevant case \n",
    "beans = search_articles(model, \n",
    "                          \"I like to eat beans\", \n",
    "                          combined_embeddings, \n",
    "                          meta, \n",
    "                          5,\n",
    "                          0.3,\n",
    "                          sort_key = 'relevance_score')\n",
    "\n",
    "beans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
